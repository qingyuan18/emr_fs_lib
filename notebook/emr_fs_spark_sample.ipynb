{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c73736",
   "metadata": {},
   "source": [
    "## 0.1.Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665126ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/spark-tfrecord_2.12-0.3.0.jar, hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '4G', 'spark.executor.cores': 2, 'spark.dynamicAllocation.initialExecutors': 2}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/spark-tfrecord_2.12-0.3.0.jar, hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"4G\",\n",
    "             \"spark.executor.cores\": 2,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":2,\n",
    "             \"spark.sql.hive.convertMetastoreParquet\": \"false\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eda668",
   "metadata": {},
   "source": [
    " ## 0.2 Hudi configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3f8765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>59</td><td>application_1639355815254_1097</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-83.ap-southeast-1.compute.internal:20888/proxy/application_1639355815254_1097/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-28.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1639355815254_1097_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "HIVE_DATABASE_OPT_KEY=\"hoodie.datasource.hive_sync.database\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "MERGE_SCHEME=\"hoodie.mergeSchema\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\"\n",
    "\n",
    "hudi_options={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a494d5e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------------+--------+\n",
      "|customer_id|city_code|state_code|country_code|dt      |\n",
      "+-----------+---------+----------+------------+--------+\n",
      "|109367     |2        |5         |3           |20210211|\n",
      "|573291     |1        |49        |2           |20210211|\n",
      "|828412     |4        |31        |3           |20210307|\n",
      "|828415     |3        |5         |2           |20210307|\n",
      "|828413     |3        |31        |1           |20210307|\n",
      "|828414     |3        |31        |5           |20210307|\n",
      "|109382     |2        |40        |2           |20210212|\n",
      "|109366     |2        |40        |4           |20210312|\n",
      "|109365     |2        |5         |5           |20210311|\n",
      "|124013     |4        |5         |2           |20210305|\n",
      "|828400     |3        |31        |2           |20210304|\n",
      "|124014     |4        |31        |1           |20210306|\n",
      "+-----------+---------+----------+------------+--------+"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"drop table emr_feature_store.customer_base\")\n",
    "#spark.sql(\"drop table emr_feature_store.customer_advance\")\n",
    "#spark.sql(\"drop database emr_feature_store\")\n",
    "#spark.sql(\"show create table emr_feature_store.customer_base as serde\").show(10000,False)\n",
    "#spark.sql(\"select customer_id,city_code,state_code,country_code,dt  from emr_feature_store.customer_base \").show(10000,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269477e",
   "metadata": {},
   "source": [
    "## 1 create emr feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f29dd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "##########feature store 相关#########\n",
    "feature_store_name='emr_feature_store'\n",
    "feature_store_s3_path='s3://emrfssampledata/emr_feature_store/'\n",
    "#####################################\n",
    "sc = SparkSession.builder\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()\n",
    "sql=\"create database if not exists \"+feature_store_name+\" location '\"+feature_store_s3_path+\"'\"\n",
    "sc.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fd6b9",
   "metadata": {},
   "source": [
    "## 2 create emr feature group\n",
    "note： in emr 6.5, we can use spark sql to directly create hudi table (\"using hudi\"), no need to create and sychronize hive metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b50d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql==CREATE external TABLE if not exists customer_base( \n",
      "customer_id int, city_code   int, state_code  string ,country_code string, dt string)  \n",
      "PARTITIONED BY ( \n",
      "  dt) \n",
      "ROW FORMAT SERDE \n",
      "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
      "STORED AS INPUTFORMAT \n",
      "  'org.apache.hudi.hadoop.HoodieParquetInputFormat' \n",
      "OUTPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' \n",
      "TBLPROPERTIES ('feature_unique_key'='customer_id','feature_partition_key'='dt')\n",
      "LOCATION 's3://emrfssampledata/emr_feature_store/customer_base'\n",
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "##########feature store 相关#########\n",
    "spark.sql(\"use \"+feature_store_name+\";\")\n",
    "feature_group_name = \"customer_base\"\n",
    "feature_unique_key = 'customer_id'\n",
    "feature_partition_key = 'dt'\n",
    "feature_group_path = feature_store_s3_path+feature_group_name\n",
    "features=\"customer_id int, city_code   int, state_code  string ,country_code string, dt string\"\n",
    "\n",
    "#feature_group_name = \"customer_advance\"\n",
    "#feature_unique_key = 'customer_id'\n",
    "#feature_partition_key = 'dt'\n",
    "#feature_group_path = feature_store_s3_path+feature_group_name\n",
    "#features=\"customer_id int\",age int,diabetes string, ejection_fraction string,high_blood_pressure string,platelets string,sex string, smoking string,death_event string,dt  string\n",
    "#####################################\n",
    "sql=\"\"\"CREATE external TABLE if not exists {0}( \n",
    "{4})  \n",
    "PARTITIONED BY ( \n",
    "  dt) \n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hudi.hadoop.HoodieParquetInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' \n",
    "TBLPROPERTIES ('feature_unique_key'='{1}','feature_partition_key'='{2}')\n",
    "LOCATION '{3}'\"\"\"\n",
    "sql=sql.format(feature_group_name,feature_unique_key,feature_partition_key,feature_group_path,features)\n",
    "print(\"sql==\"+sql)\n",
    "spark.sql(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc15b8",
   "metadata": {},
   "source": [
    "## 3 ingestion data into feature group \n",
    "note :provide feature partition key and feature primary key\n",
    "#feature_unique_key,特征唯一标识列，比如：uuid \n",
    "#feature_partition_key, 特征group表分区字段，比如dt\n",
    "#hoodie.datasource.write.operation, 操作特征group表方式，设置为upsert支持update\n",
    "#hoodie.mergeSchema,设置为\"true\" 支持新增feature field\n",
    "#只有第一次创建表才使用overwrite，其他情况都用append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1b9965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hoodie.table.name': 'customer_base', 'hoodie.datasource.write.recordkey.field': 'customer_id', 'hoodie.datasource.write.partitionpath.field': 'dt', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.precombine.field': 'dt', 'hoodie.mergeSchema': 'true', 'hoodie.datasource.hive_sync.table': 'customer_base', 'hoodie.datasource.hive_sync.enable': 'true', 'hoodie.datasource.hive_sync.partition_fields': 'dt', 'hoodie.datasource.hive_sync.database': 'emr_feature_store'}"
     ]
    }
   ],
   "source": [
    "# 定义源数据路径。\n",
    "dataGen = \"s3://emrfssampledata/feature_store_customer_base.csv\"\n",
    "#dataGen = \"s3://emrfssampledata/feature_store_customer_advance.csv\"\n",
    "df=sc.read.format(\"csv\").option(\"header\", \"true\").load(dataGen)\n",
    "\n",
    "##########feature store 相关#########\n",
    "feature_group_name = 'customer_base'\n",
    "feature_unique_key = 'customer_id'\n",
    "feature_partition_key='dt'\n",
    "basePath = feature_store_s3_path+\"/\"+feature_group_name\n",
    "#####################################\n",
    "\n",
    "# hudi操作配置预置参数，\n",
    "hudi_options = {\n",
    "   TABLE_NAME: feature_group_name,\n",
    "   RECORDKEY_FIELD_OPT_KEY: feature_unique_key,\n",
    "   PARTITIONPATH_FIELD_OPT_KEY: feature_partition_key,\n",
    "   OPERATION_OPT_KEY: UPSERT_OPERATION_OPT_VAL,\n",
    "   PRECOMBINE_FIELD_OPT_KEY: feature_partition_key,\n",
    "   MERGE_SCHEME: \"true\",\n",
    "   HIVE_TABLE_OPT_KEY:feature_group_name,\n",
    "   HIVE_SYNC_ENABLED_OPT_KEY:\"true\",\n",
    "   HIVE_PARTITION_FIELDS_OPT_KEY:feature_partition_key,\n",
    "   HIVE_DATABASE_OPT_KEY:feature_store_name\n",
    "  #'hoodie.datasource.write.table.name': feature_group_name,  \n",
    "  #HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY:NONPARTITION_EXTRACTOR_CLASS_OPT_VAL,\n",
    "  #KEYGENERATOR_CLASS_OPT_KEY:NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL\n",
    "}\n",
    "\n",
    "print(hudi_options)\n",
    "df=df.withColumn(\"customer_id\",df[\"customer_id\"].cast(\"int\"))\n",
    "df=df.withColumn(\"city_code\",df[\"city_code\"].cast(\"int\"))\n",
    "df.write.format(\"org.apache.hudi\"). \\\n",
    "  options(**hudi_options). \\\n",
    "  mode(\"overwrite\"). \\\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b04dcb",
   "metadata": {},
   "source": [
    "## 4 add new feature in feature group \n",
    "#标准sql add column查询操作\n",
    "#hudi schemaMerge参数默认打开，写入自动会合并分区parquet中的schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438fc54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------------+--------+-------------+\n",
      "|customer_id|city_code|state_code|country_code|      dt|identify_code|\n",
      "+-----------+---------+----------+------------+--------+-------------+\n",
      "|     109367|        2|         5|           3|20210211|  51109873794|\n",
      "|     573291|        1|        49|           2|20210211|  51109873783|\n",
      "|     828412|        4|        31|           3|20210307|  51109873788|\n",
      "|     828415|        3|         5|           2|20210307|         null|\n",
      "|     828413|        3|        31|           1|20210307|  51109873789|\n",
      "|     828414|        3|        31|           5|20210307|  51109873790|\n",
      "|     109382|        2|        40|           2|20210212|  51109873784|\n",
      "|     828400|        3|        31|           2|20210304|         null|\n",
      "|     124014|        4|        31|           1|20210306|         null|\n",
      "|     109366|        2|        40|           4|20210312|  51109873793|\n",
      "|     109365|        2|         5|           5|20210311|         null|\n",
      "|     124013|        4|         5|           2|20210305|         null|\n",
      "+-----------+---------+----------+------------+--------+-------------+"
     ]
    }
   ],
   "source": [
    "################### add feature column##########################\n",
    "\n",
    "##########feature store 相关#########\n",
    "new_feature=\"identify_code string\"\n",
    "sc.sql(\"alter table emr_feature_store.customer_base add COLUMNS  (\"+new_feature+\")\")\n",
    "#####################################\n",
    "\n",
    "dataGen = \"s3://emrfssampledata/feature_store_customer_base_update.csv\"\n",
    "df_updates=sc.read.format(\"csv\").option(\"header\", \"true\").load(dataGen)\n",
    "df_updates=df_updates.withColumn(\"customer_id\",df_updates[\"customer_id\"].cast(\"int\"))\n",
    "df_updates=df_updates.withColumn(\"city_code\",df_updates[\"city_code\"].cast(\"int\"))\n",
    "df_updates.write.format(\"org.apache.hudi\"). \\\n",
    "  options(**hudi_options). \\\n",
    "  mode(\"Append\"). \\\n",
    "  save(basePath)\n",
    "sc.sql(\"select customer_id,city_code,state_code,country_code,dt,identify_code from emr_feature_store.customer_base\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117d6d1",
   "metadata": {},
   "source": [
    "## 5 upsert feature group \n",
    "#hudi操作配置预置参数修改为upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e1d491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hudi_options[OPERATION_OPT_KEY]=UPSERT_OPERATION_OPT_VAL\n",
    "##########feature store 相关#########\n",
    "updates = [(124014, 100000,\"40\",\"4\",\"20210306\",\"51100212121212\")]\n",
    "#####################################\n",
    "\n",
    "df_updates=spark.createDataFrame(updates, schema=[\"customer_id\",\"city_code\",\"state_code\",\"country_code\",\"dt\",\"identify_code\"])\n",
    "df_updates=df_updates.withColumn(\"customer_id\",df_updates[\"customer_id\"].cast(\"int\"))\n",
    "df_updates=df_updates.withColumn(\"city_code\",df_updates[\"city_code\"].cast(\"int\"))\n",
    "df_updates.write.format(\"org.apache.hudi\"). \\\n",
    "  options(**hudi_options). \\\n",
    "  mode(\"append\"). \\\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac30b2",
   "metadata": {},
   "source": [
    "## 6 time travel & filter & join query feature group \n",
    "#增量查询，设置hoodie_commit_time起始时间\n",
    "#标准sql filter & join查询操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c1f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------------+--------+-------------+\n",
      "|customer_id|city_code|state_code|country_code|      dt|identify_code|\n",
      "+-----------+---------+----------+------------+--------+-------------+\n",
      "|     109366|        2|        40|           4|20210312|  51109873793|\n",
      "|     109382|        2|        40|           2|20210212|  51109873784|\n",
      "|     109365|        2|         5|           5|20210311|         null|\n",
      "|     109367|        2|         5|           3|20210211|  51109873794|\n",
      "+-----------+---------+----------+------------+--------+-------------+\n",
      "\n",
      "+-----------+---------+----------+------------+--------+\n",
      "|customer_id|city_code|state_code|country_code|      dt|\n",
      "+-----------+---------+----------+------------+--------+\n",
      "|     124014|   100000|        40|           4|20210306|\n",
      "|     109382|        2|        40|           2|20210212|\n",
      "|     828412|        4|        31|           3|20210307|\n",
      "|     828415|        3|         5|           2|20210307|\n",
      "|     828413|        3|        31|           1|20210307|\n",
      "|     828414|        3|        31|           5|20210307|\n",
      "|     109365|        2|         5|           5|20210311|\n",
      "|     828400|        3|        31|           2|20210304|\n",
      "|     109366|        2|        40|           4|20210312|\n",
      "|     124013|        4|         5|           2|20210305|\n",
      "|     109367|        2|         5|           3|20210211|\n",
      "|     573291|        1|        49|           2|20210211|\n",
      "+-----------+---------+----------+------------+--------+"
     ]
    }
   ],
   "source": [
    "##############time travel版本查询#####################################\n",
    "beginTime = '20220112000000'\n",
    "endTime = '20220122235959'\n",
    "\n",
    "spark.sql(\"select customer_id,city_code,state_code,country_code,dt,identify_code from emr_feature_store.customer_base where city_code=2 \").show()\n",
    "spark.sql(\"select customer_id,city_code,state_code,country_code,dt from  emr_feature_store.customer_base where _hoodie_commit_time>'\"+beginTime\\\n",
    "          +\"' and _hoodie_commit_time <'\"+endTime+\"'\").show()\n",
    "#spark.sql(\"select a.customer_id,a.city_code,a.country_code,a.dt,a.identify_code,b.age,b.platelets,b.sex from  emr_feature_store.customer_base a left join emr_feature_store.customer_advance b where a.customer_id = b.customer_id and a._hoodie_commit_time>'\"+beginTime\\\n",
    "#          +\"' and a._hoodie_commit_time <'\"+endTime+\"'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca573b1",
   "metadata": {},
   "source": [
    "## 7 train dataset retrive \n",
    "#支持tfrecord、libsvm，csv\n",
    "#sql 查询后dataframe操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f6e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############retrieve 特征group数据做为train dataset######################\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "df=spark.sql(\"select customer_id,city_code,state_code,country_code,identify_code,dt from  emr_feature_store.customer_base where dt >'20210301'\")\n",
    "format='tfrecord'\n",
    "path=\"s3://emrfssampledata/traindataset/output\"\n",
    "if format == 'tfrecord':\n",
    "    path=path+\".tfrecord\"\n",
    "    df.write.format(\"tfrecord\").option(\"recordType\", \"Example\").mode(\"overwrite\").save(path)\n",
    "elif format == 'libsvm':\n",
    "    path=path+\".libsvm\"\n",
    "    convDf = df.rdd.map(lambda line: LabeledPoint(line[0],[line[1:]]))\n",
    "    MLUtils.saveAsLibSVMFile(convDf, path)\n",
    "elif format == 'csv':\n",
    "    path=path+\".csv\"\n",
    "    df.write.format(\"csv\").mode(\"overwrite\").save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
